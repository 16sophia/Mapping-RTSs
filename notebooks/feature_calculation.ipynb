{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import warnings\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import rasterio \n",
    "\n",
    "from rasterio.features import geometry_mask\n",
    "from PIL import Image\n",
    "from scipy.ndimage import binary_erosion\n",
    "from scipy.ndimage import binary_dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose input parameters: \n",
    "name = \"Sophia\" # Kathrin, Philipp\n",
    "site = \"kolguev\" # kolguev, taymyr, herschel, tuktoyaktuk, Peel\n",
    "mode_dilation_erosion = 'none' # erosion, dilation, none: none calulates the actual volume\n",
    "pixelresolution = 10 # 10 m for difference DEM generated with method 3, 6 m for method 1\n",
    "# structuring element for dilation / erosion\n",
    "structuring_element = np.array([[0, 1, 0],\n",
    "                                      [1, 1, 1],\n",
    "                                      [0, 1, 0]], dtype=bool)\n",
    "\n",
    "# Set path to data\n",
    "local_folder = os.getcwd()\n",
    "file_folder = r'C:\\Users\\sophi\\Documents\\thesis\\code\\comparing labels\\labeled_data'  # path to polygons extracted from QGIS labelling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sophi\\anaconda3\\envs\\geoprocessing\\lib\\site-packages\\geopandas\\geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "c:\\Users\\sophi\\anaconda3\\envs\\geoprocessing\\lib\\site-packages\\geopandas\\geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n",
      "c:\\Users\\sophi\\anaconda3\\envs\\geoprocessing\\lib\\site-packages\\geopandas\\geoseries.py:645: FutureWarning: the convert_dtype parameter is deprecated and will be removed in a future version.  Do ``ser.astype(object).apply()`` instead if you want ``convert_dtype=False``.\n",
      "  result = super().apply(func, convert_dtype=convert_dtype, args=args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Read QGIS data--------------------------------------- \n",
    "if site == \"Peel\":\n",
    "    network_folder = r'Z:\\Kathrin\\dem_processing\\dem_differencing\\method_1'  # UNC path to dem tiles\n",
    "    tile_path = os.path.join(local_folder, network_folder) # Construct the full path to the network folder from the local directory\n",
    "    # Extract tile names and polygon\n",
    "    tile_name = [item for item in os.listdir(os.path.join(tile_path))]\n",
    "    year_start_ = [2010, 2011]\n",
    "    year_end = [2016, 2021]\n",
    "elif site == \"herschel\":\n",
    "    network_folder = r'C:\\Users\\sophi\\Documents\\thesis\\code\\comparing labels\\labeled_data\\herschel'  # UNC path to dem tiles\n",
    "    tile_path = os.path.join(local_folder, network_folder)\n",
    "    tile_name = ['tile_18_42_7_6']\n",
    "    year_start_ = [2010]\n",
    "    year_end = [2016, 2018]\n",
    "elif site == \"kolguev\":\n",
    "    network_folder = r'C:\\Users\\sophi\\Documents\\thesis\\code\\comparing labels\\labeled_data\\kolguev'  # UNC path to dem tiles\n",
    "    tile_path = os.path.join(local_folder, network_folder)\n",
    "    tile_name = ['tile_63_42_7_4']\n",
    "    year_start_ = [2010]\n",
    "    year_end = [2017]\n",
    "\n",
    "elif site == \"taymyr\":\n",
    "    network_folder = r'C:\\Users\\sophi\\Documents\\thesis\\code\\comparing labels\\labeled_data\\taymyr_south_1'  # UNC path to dem tiles\n",
    "    tile_path = os.path.join(local_folder, network_folder)\n",
    "    tile_name = ['tile_54_53_2_6', 'tile_54_53_4_9']\n",
    "    year_start = [2010]\n",
    "    year_end = [2016, 2020]\n",
    "\n",
    "elif site == \"tuktoyaktuk\":\n",
    "    network_folder = r'C:\\Users\\sophi\\Documents\\thesis\\code\\comparing labels\\labeled_data\\tuktoyaktuk'  # UNC path to dem tiles\n",
    "    tile_path = os.path.join(local_folder, network_folder)\n",
    "    tile_name = ['tile_18_40_2_6', 'tile_18_40_4_8']\n",
    "    year_start_ = [2010]\n",
    "    year_end = [2016, 2021]\n",
    "\n",
    "\n",
    "polygons = gpd.read_file(os.path.join(local_folder, file_folder, f\"{site}_{name}.geojson\")) # polygons extracted from QGIS labelling process\n",
    "\n",
    "# Calculate shape attributes---------------------------------------\n",
    "# Calculate the area and perimeter for each geometry\n",
    "polygons['area'] = polygons['geometry'].apply(lambda x: x.area)\n",
    "polygons['perimeter'] = polygons['geometry'].apply(lambda x: x.length)\n",
    "\n",
    "# Calculate circularity using precomputed area and perimeter\n",
    "polygons['circularity'] = (4 * np.pi * polygons['area']) / (polygons['perimeter'] ** 2)\n",
    "\n",
    "# Calculate solidity using precomputed area and convex hull area\n",
    "polygons['solidity'] = polygons['area'] / polygons['geometry'].apply(lambda x: x.convex_hull.area)\n",
    "\n",
    "polygons[\"diameter_max\"] = 2 * shapely.minimum_bounding_radius(polygons[\"geometry\"])\n",
    "polygons[\"n_vertices\"] = shapely.get_num_coordinates(polygons[\"geometry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed year from str to int\n"
     ]
    }
   ],
   "source": [
    "# Fix wrong columnname\n",
    "if site == 'tuktoyaktuk':\n",
    "    polygons.rename(columns={'tile_range': 'tile_name'}, inplace=True)\n",
    "\n",
    "# change start year and end to int\n",
    "polygons['year_start'] = polygons['year_start'].astype(int)\n",
    "polygons['year_end'] = polygons['year_end'].astype(int)\n",
    "print(\"Changed year from str to int\")\n",
    "year_start = list(polygons['year_start'].unique())\n",
    "year_end = list(polygons['year_end'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 RTS appear more than once in df (Because it overlaps multiple tiles). Each id is a unique RTS, we have 15 unique RTS\n"
     ]
    }
   ],
   "source": [
    "# Check number of unique RTSs\n",
    "unique = polygons[\"id\"].drop_duplicates(keep=False)\n",
    "boolean_unique = polygons[\"id\"].isin(unique)\n",
    "non_unique = polygons[\"id\"][~boolean_unique]\n",
    "print(f\"{len(non_unique)} RTS appear more than once in df (Because it overlaps multiple tiles). Each id is a unique RTS, we have {len(unique)} unique RTS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RTS attributes based on (normalized) difference DEM\n",
    "# Dictionary that keeps track of RTS that appear in multiple images: key = RTS index in polygon df\n",
    "# dictionary = key accesses list of dictionary where each dictionary is metric from an image\n",
    "RTS_multiple = {}\n",
    "RTS_checked = []\n",
    "stop_ = False\n",
    "\n",
    "# iterate through tiles------------------------------------------------------------------------\n",
    "for tile in tile_name:\n",
    "\n",
    "    # Extract sub_df according to tilename\n",
    "    sub_polygons_image = polygons[polygons[\"tile_name\"] ==  tile]\n",
    "    #print(len(sub_polygons_image))\n",
    "    if len(sub_polygons_image)>0:\n",
    "        # iterate through all combination of years------------------------------------------------------------------------\n",
    "        # Makes sure that all tiles are read\n",
    "        for start in year_start:\n",
    "            for end in year_end:\n",
    "                sub_polygons = sub_polygons_image[(sub_polygons_image[\"year_start\"] == start) & (sub_polygons_image[\"year_end\"] == end)]\n",
    "                #print('y_start', start, 'year_end', end, 'tile', tile)\n",
    "                # Only read image if there is at least one RTS in image to avoid unnecessary tasks\n",
    "                \n",
    "                if len(sub_polygons)>0:\n",
    "                    # Read corresponding norm dem image---------- \n",
    "                    # Extract dem name corresponding to tile, start, end dem\n",
    "                    if site == 'Peel' or site == 'taymyr' or site == 'tuktoyaktuk':\n",
    "                        norm_dem_name = [item for item in os.listdir(os.path.join(tile_path, tile))\n",
    "                                                if item.startswith(\"norm_diff_dem\") and item.endswith(\".tif\") and str(start) in item and str(end) in item]\n",
    "                        diff_dem_name = [item for item in os.listdir(os.path.join(tile_path, tile))\n",
    "                                                if item.startswith(\"diff_dem\") and item.endswith(\".tif\") and str(start) in item and str(end) in item]\n",
    "                    else:\n",
    "                        norm_dem_name = [item for item in os.listdir(tile_path)\n",
    "                                                if item.startswith(\"norm_diff_dem\") and item.endswith(\".tif\") and str(start) in item and str(end) in item]\n",
    "                        diff_dem_name = [item for item in os.listdir(os.path.join(tile_path))\n",
    "                                                if item.startswith(\"diff_dem\") and item.endswith(\".tif\") and str(start) in item and str(end) in item]\n",
    "                    if len(norm_dem_name)>1:\n",
    "                        warnings.warn(\"There are more than one image that correspond to the wanted tile. The first\", UserWarning)\n",
    "\n",
    "                    if len(norm_dem_name)<1:\n",
    "                        warnings.warn(f\"RTS is in a non existing {tile, start, end}. Skip RTS calculations\", UserWarning)\n",
    "                        continue\n",
    "                        \n",
    "                    #print(norm_dem_name)\n",
    "                    # Read image as georeferenced tif\n",
    "                    # Read norm dem\n",
    "                    if site == 'Peel'or site == 'taymyr' or site == 'tuktoyaktuk': # Split into subfolders\n",
    "                        norm_dem_path = os.path.join(tile_path, tile, norm_dem_name[0]) \n",
    "                        diff_dem_path = os.path.join(tile_path, tile, diff_dem_name[0]) \n",
    "                        #print(norm_dem_path)\n",
    "                    else: # All in one folder\n",
    "                        norm_dem_path = os.path.join(tile_path, norm_dem_name[0])\n",
    "                        diff_dem_path = os.path.join(tile_path, diff_dem_name[0]) \n",
    "                    src = rasterio.open(norm_dem_path)\n",
    "                    norm_dem = src.read(1)\n",
    "                    transform_norm = src.transform #transformation from pixel coordinates of source to the coordinate system of the input shapes\n",
    "\n",
    "                    src_diff = rasterio.open(diff_dem_path)\n",
    "                    diff_dem = src_diff.read(1)\n",
    "                    transform_diff = src_diff.transform #transformation from pixel coordinates of source to the coordinate system of the input shapes\n",
    "\n",
    "                    # Iterate through all RTS that are within the image-----------------------------------------------------------------------\n",
    "                    for index, row in sub_polygons.iterrows():    \n",
    "                        # Intersect polygons with norm dem image---------- \n",
    "                        geom = row['geometry']\n",
    "                        mask = geometry_mask([geom], out_shape=norm_dem.shape, transform = transform_norm, invert=True) # invert to make sure that mask is 1 and background = 0\n",
    "                        norm_dem_values = norm_dem[mask]\n",
    "\n",
    "                        if mode_dilation_erosion =='dilation':\n",
    "                            mask_diff = geometry_mask([geom], out_shape=diff_dem.shape, transform = transform_diff, invert=True) # invert to make sure that mask is 1 and background = 0\n",
    "                            mask_diff = binary_dilation(mask_diff, structure=structuring_element)\n",
    "                            diff_dem_values = diff_dem[mask_diff]   \n",
    "                            area = np.sum(mask_diff) *(pixelresolution*pixelresolution)\n",
    "                        elif  mode_dilation_erosion =='erosion':\n",
    "                            mask_diff = geometry_mask([geom], out_shape=diff_dem.shape, transform = transform_diff, invert=True) # invert to make sure that mask is 1 and background = 0\n",
    "                            mask_diff = binary_erosion(mask_diff, structure=structuring_element)\n",
    "                            diff_dem_values = diff_dem[mask_diff]  \n",
    "                            area = np.sum(mask_diff) *(pixelresolution*pixelresolution)\n",
    "                        else:\n",
    "                            # Intersect RTS polygon with difference DEM: original shape and buffered shape to take lower volume into account\n",
    "                            mask_diff = geometry_mask([geom], out_shape=diff_dem.shape, transform = transform_diff, invert=True) # invert to make sure that mask is 1 and background = 0\n",
    "                            diff_dem_values = diff_dem[mask_diff]     \n",
    "                            area =row['area']                 \n",
    "                        \n",
    "                        # Calculate metrics\n",
    "                        norm_mean = np.nanmean(norm_dem_values)\n",
    "                        norm_std = np.nanstd(norm_dem_values)\n",
    "                        norm_var = np.nanvar(norm_dem_values)\n",
    "                        norm_median = np.nanmedian(norm_dem_values)\n",
    "                        norm_coeff_var = norm_std / norm_var\n",
    "                        norm_n_pixel = norm_dem_values.size\n",
    "                        if not norm_n_pixel >0: # ensure that RTS is > 0\n",
    "                            print('stop')\n",
    "                            stop_ = True\n",
    "                            break\n",
    "\n",
    "                        # Calculate volume\n",
    "                        height_mean = np.nanmean(diff_dem_values)\n",
    "                        volume = area * height_mean\n",
    "\n",
    "                        # Check if RTS has already been checked. If yes -> RTS is on multple tiles: add values to dictionary: key = RTS_id, values = intensity metric\n",
    "                        # ---> If RTS appears in multiple picitures, we add values into a dictionary   \n",
    "                        RTS_id = polygons.at[index, \"id\"]\n",
    "\n",
    "                        if RTS_id in RTS_checked:\n",
    "                            metric_dict = {\n",
    "                                \"norm_mean\": norm_mean,\n",
    "                                \"norm_std\": norm_std, \n",
    "                                \"norm_var\": norm_var,\n",
    "                                \"norm_median\":  norm_median,\n",
    "                                \"norm_coeff_var\": norm_coeff_var,\n",
    "                                \"norm_n_pixel\": norm_n_pixel,\n",
    "                                \"volume\": volume                        \n",
    "                            }\n",
    "\n",
    "                            # Check if the RTS_id exists in the dictionary (is already a  key): RTS is in > 2 pictures. Add 3rd image as another dictionary\n",
    "                            if RTS_id in RTS_multiple:\n",
    "                                RTS_multiple[RTS_id].append(metric_dict)\n",
    "                            else:\n",
    "                                RTS_multiple[RTS_id] = [metric_dict]\n",
    "                            \n",
    "                        # RTS has not been checked before: add values into polygon df\n",
    "                        else:\n",
    "                            polygons.at[index, \"norm_mean\"] =norm_mean\n",
    "                            polygons.at[index, \"norm_std\"] = norm_std\n",
    "                            polygons.at[index, \"norm_var\"] = norm_var\n",
    "                            polygons.at[index, \"norm_median\"] = norm_median\n",
    "                            polygons.at[index, \"norm_coeff_var\"] = norm_coeff_var\n",
    "                            polygons.at[index, \"norm_n_pixel\"] = norm_n_pixel\n",
    "                            polygons.at[index, \"volume\"] = volume\n",
    "\n",
    "                        # Mark RTS as checked\n",
    "                        RTS_checked.append(polygons.at[index, \"id\"])\n",
    "                if stop_:\n",
    "                    break\n",
    "            if stop_:\n",
    "                break\n",
    "    if stop_:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some RTS appear in multiple tiles -> For intensity metrics we calculate the weighted mean for each instance\n",
    "# First, all values of a RTS have to be combined from RTS_multiple dictionary and polygons df\n",
    "# RTS_multiple dictionary contains metric values of RTS instances that appear for the second, third... times. First appearance of RTS is saved in polygons df\n",
    "\n",
    "polygons_aggregated = polygons.copy(deep=True)\n",
    "# Extract RTS id that appear in multiple tiles\n",
    "id_multiple = list(RTS_multiple.keys())\n",
    "\n",
    "# Iterate through RTS that appear in multiple tiles\n",
    "for id_RTS_m in id_multiple:\n",
    "    # Create an empty aggregation df and fill first instance with first appearance of RTS from polygons df\n",
    "    # Extract intensity metrics (polygons_aggregated.columns[12:]) from corresponding RTS\n",
    "    same_RTS = polygons_aggregated[polygons_aggregated[\"id\"]== id_RTS_m][polygons_aggregated.columns[12:]] \n",
    "    n_multiple = len(RTS_multiple[id_RTS_m]) + 1 # Get number of same instance in different tiles. +1 because first instance of RTS is saved in polygons df\n",
    "    aggregation = pd.DataFrame(None,  index=range(n_multiple), columns=polygons_aggregated.columns[12:])\n",
    "    aggregation.iloc[0] = same_RTS.iloc[0]\n",
    "\n",
    "    # Fill other appearance of RTS from RTS_multiple dictionary\n",
    "    for i in range (1, n_multiple): # First RTS is already assigned in df. Dictionary only contains value from second RTS instance on\n",
    "        RTS_i_dict = RTS_multiple[id_RTS_m][i-1] \n",
    "        # Save all values from dictionary to df\n",
    "        for col, value in RTS_i_dict.items(): \n",
    "            aggregation.at[i, col] = value\n",
    "        # Calculate weighted mean and insert back into polygons_aggregated df\n",
    "        aggregated = np.average(aggregation, axis = 0, weights = aggregation[\"norm_n_pixel\"])\n",
    "        polygons_aggregated.loc[polygons_aggregated['id'] == id_RTS_m, list(polygons_aggregated.columns[12:])] =aggregated\n",
    "# Remove rows where id is not unique\n",
    "polygons_aggregated = polygons_aggregated[~polygons_aggregated['id'].duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 RTS appear more than once in df (Because it overlaps multiple tiles). Each id is a unique RTS, we have 99 unique RTS\n"
     ]
    }
   ],
   "source": [
    "unique = polygons_aggregated[\"id\"].drop_duplicates(keep=False)\n",
    "boolean_unique = polygons_aggregated[\"id\"].isin(unique)\n",
    "non_unique = polygons_aggregated[\"id\"][~boolean_unique]\n",
    "print(f\"{len(non_unique)} RTS appear more than once in df (Because it overlaps multiple tiles). Each id is a unique RTS, we have {len(unique)} unique RTS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where intensity has not yet been calculated and remove polygons that are not in Peel\n",
    "polygons_save = polygons_aggregated[polygons_aggregated[\"norm_var\"].notna()]\n",
    "# check length\n",
    "print(len(np.unique(polygons_save.id)))\n",
    "# save\n",
    "#polygons_save.to_csv(f'{site}_{name}.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "[2010] [2021 2016]\n",
      "1465625.0\n"
     ]
    }
   ],
   "source": [
    "data_ = polygons_save #pd.read_csv(f\"{site}_{name}.csv\")\n",
    "print(data_.year_start.unique(), data_.year_end.unique())\n",
    "print('total volume: ', np.round(data_.volume.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2010] [2021 2016]\n"
     ]
    }
   ],
   "source": [
    "end_ = data_.year_end.unique()\n",
    "data1 = data_[(data_.year_end == end_[0])] # | (data_.year_end == end_[2]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volume 2215977.0\n",
      "mean area 5108.0\n"
     ]
    }
   ],
   "source": [
    "print('volume with end',end_[0], 'is:', np.round(data1.volume.sum()))\n",
    "print('mean area', np.round(data1.area.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maskrcnn_lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
