{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8732b56-e72e-4a5b-aea0-1a4d3b78f70e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import src.utils as utils\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "from src.transforms import clipped_zoom\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e00ea-4dae-433d-a9bb-d8001de21414",
   "metadata": {},
   "source": [
    "### Split train-val and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d5046-6a8f-4481-9a73-cb4b9bd96b94",
   "metadata": {},
   "source": [
    "Split data into (train & validaiton) and test with 0.8:0.2 ratio.\n",
    "Clean the data to remove entries with invalid objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77998d73-f30a-4d4a-96bc-667a9766ba3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split between train and test for mask and image in raw data\n",
    "name = \"Sophia\"\n",
    "# Move fraction of train data to test folder\n",
    "source_directory = f\"data/{name}/data_train/data_original/masks\"\n",
    "source_directory2 = f\"data/{name}/data_train/data_original/images\"\n",
    "\n",
    "destination_directory = f\"data/{name}/data_test/masks\"\n",
    "destination_directory2 = f\"data/{name}/data_test/images\"\n",
    "fraction_to_move = 0.2\n",
    "\n",
    "split_train_test_mask(source_directory, destination_directory, source_directory2, destination_directory2, fraction_to_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d8a63-41bc-4fea-8bf1-6df1e95fe245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move all data to clean data before cleaning\n",
    "# Data path to image and masks\n",
    "data_path_raw = f'data/{name}/data_raw/data_original/data_train'\n",
    "test_path_raw = f'data/{name}/data_raw/data_original/data_test'\n",
    "data_path = f'data/{name}/data_clean/data_original/data_train'\n",
    "test_path = f'data/{name}/data_clean/data_original/data_test'\n",
    "\n",
    "move_images(data_path_raw, data_path, all_ = True)\n",
    "move_images(test_path_raw, test_path, all_ = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ad839-3a11-41d9-85c1-e95643c01845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "dataset = DEMDataset(data_path_raw, n_channel = 1, channel_list = [\"images\"])\n",
    "test_set = DEMDataset(test_path_raw, n_channel = 1, channel_list = [\"images\"])\n",
    "original_data = DEMDataset(data_path_raw, n_channel = 1, channel_list = [\"images\"])\n",
    "original_test = DEMDataset(test_path_raw, n_channel = 1, channel_list = [\"images\"])\n",
    "# Clean data and generate clean file directory \n",
    "dataset, test_set = utils.clean_raw_data(dataset, test_set, original_data, original_test, test_path, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "5762e9fa-8425-412e-92a0-798ffe30913e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add slope, aspect data and split into train/ test\n",
    "# Move / split slope and aspect data according to mask and image directories\n",
    "# List all names in cleaned directory\n",
    "tile_train = [item for item in os.listdir(os.path.join(data_path, \"masks\"))\n",
    "                           if item.endswith(\".tif\")]\n",
    "check_train = [item for item in os.listdir(os.path.join(data_path, \"images\"))\n",
    "                           if item.endswith(\".tif\")]\n",
    "tile_test = [item for item in os.listdir(os.path.join(test_path, \"masks\"))\n",
    "                           if item.endswith(\".tif\")]\n",
    "check_test = [item for item in os.listdir(os.path.join(test_path, \"images\"))\n",
    "                           if item.endswith(\".tif\")]\n",
    "if (check_train != tile_train) | (check_test != tile_test):\n",
    "    print(\"Warning, data has wrongly been added to the clean directory. Mask and image tiles do not correspond. Repeat data splitting and cleaning part\")\n",
    "\n",
    "# Slope train\n",
    "slope_path_raw = f\"data/{name}/data_raw/slope\"\n",
    "slope_path_clean = f\"data/{name}/data_clean/data_train/slope\"\n",
    "move_images(slope_path_raw, slope_path_clean, all_ = False, to_move = tile_train)\n",
    "\n",
    "# slope test\n",
    "slope_test_clean = f\"data/{name}/data_clean/data_test/slope\"\n",
    "move_images(slope_path_raw, slope_test_clean, all_ = False, to_move = tile_test)\n",
    "\n",
    "# aspect train\n",
    "aspect_path_raw = f\"data/{name}/data_raw/aspect\"\n",
    "aspect_path_clean = f\"data/{name}/data_clean/data_train/aspect\"\n",
    "move_images(aspect_path_raw, aspect_path_clean, all_ = False, to_move = tile_train)\n",
    "\n",
    "# aspect test\n",
    "aspect_test_clean = f\"data/{name}/data_clean/data_test/aspect\"\n",
    "move_images(aspect_path_raw, aspect_test_clean, all_ = False, to_move = tile_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb40ade2-1dc6-472d-b78d-9e057401eafd",
   "metadata": {},
   "source": [
    "***\n",
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f1558538-250e-4def-ba9c-cf652b82b462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_zoomed_data(dataset, original_data, data_path):\n",
    "    '''\n",
    "    Delete dataset in data_clean folder that are invlid by using delete property generated in __getitem__ from file dictionary\n",
    "    Invalid data: images that are empty (full of 0 or NaN) or corresponding mask are empty\n",
    "    \n",
    "    Input:\n",
    "        dataset, test_set: DEM dataset generated from raw data, will be cleaned in this function through implementation in  __getitem__ \n",
    "        original_data, original_test: DEM dataset generated from raw data, uncleaned\n",
    "        data_path: Path of tobe cleaned directory\n",
    "    \n",
    "    '''\n",
    "    # Delete invalid entries from DEMDataset: __getitem__ deletes invalid entries == a one time iteration \n",
    "    for file in dataset:\n",
    "        pass\n",
    "    \n",
    "    # Calculate intersection between original data and cleaned data \n",
    "    intersection_set = set(original_data.filename_list).intersection(dataset.filename_list)\n",
    "    \n",
    "    # Find the entries that are not in the intersection\n",
    "    not_in_intersection = [entry for entry in original_data.filename_list + dataset.filename_list if entry not in intersection_set]\n",
    "    \n",
    "    # delete file form data_clean folder which are not in the intersection = data deleted in for loop above\n",
    "    for file in not_in_intersection:\n",
    "        delete_img = os.path.join(data_path, \"images\", file)\n",
    "        if os.path.exists(delete_img):\n",
    "            # Delete the file\n",
    "            os.remove(delete_img)\n",
    "            print(f\"{file} image deleted successfully.\")\n",
    "        else:\n",
    "            print(f\"{file} image not found. No deletion performed.\")\n",
    "            \n",
    "        delete_mask = os.path.join(data_path, \"masks\", file)\n",
    "        if os.path.exists(delete_mask):\n",
    "            os.remove(delete_mask)\n",
    "            print(f\"{file} mask deleted successfully.\")\n",
    "        else:\n",
    "            print(f\"{file} mask not found. No deletion performed.\")\n",
    "            \n",
    "        delete_aspect = os.path.join(data_path, \"aspect\", file)\n",
    "        if os.path.exists(delete_aspect):\n",
    "            os.remove(delete_aspect)\n",
    "            print(f\"{file} aspect deleted successfully.\")\n",
    "        else:\n",
    "            print(f\"{file} aspect not found. No deletion performed.\")\n",
    "            \n",
    "        delete_slope = os.path.join(data_path, \"slope\", file)\n",
    "        if os.path.exists(delete_slope):\n",
    "            os.remove(delete_slope)\n",
    "            print(f\"{file} slope deleted successfully.\")\n",
    "        else:\n",
    "            print(f\"{file} slope not found. No deletion performed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecb0b7aa-319e-4849-a25b-11d916c86974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate transformed data. For each transformation, a new dataset is extracted from train_set (50%). This dataset is augmented. \n",
    "\n",
    "random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "path_original = f\"data/{name}/data_clean/data_train/data_original\"\n",
    "channel_t = [\"images\", \"masks\", \"aspect\", \"slope\"]\n",
    "percentage = 0.5\n",
    "\n",
    "# Horizontal flip transformation-----------------------------------------\n",
    "tobe_transformed, _ = split_dataset_by_percentage(train_set, percentage)\n",
    "for channel in channel_t:\n",
    "    # Transform each image/ channel\n",
    "    for _, targ in tobe_transformed:\n",
    "        # Path to original image\n",
    "        path_tobe_transf = os.path.join(path_original, channel, targ['tile'])\n",
    "\n",
    "        # Path to file directory save\n",
    "        path_transformation = f\"data/{name}/data_clean/data_train/data_transformed/h_flipping\"\n",
    "        path_save = os.path.join(path_transformation, channel, targ['tile'])\n",
    "\n",
    "        # Horizontal flip & save image\n",
    "        Image.open(path_tobe_transf).transpose(Image.FLIP_LEFT_RIGHT).save(path_save, format=\"TIFF\")\n",
    "        \n",
    "# Vertical flip transformation-----------------------------------------\n",
    "tobe_transformed, _ = split_dataset_by_percentage(train_set, percentage)\n",
    "for channel in channel_t:\n",
    "    # Transform each image/ channel\n",
    "    for _, targ in tobe_transformed:\n",
    "        # Path to original image\n",
    "        path_tobe_transf = os.path.join(path_original, channel, targ['tile'])\n",
    "\n",
    "        # Path to file directory save\n",
    "        path_transformation = f\"data/{name}/data_clean/data_train/data_transformed/v_flipping\"\n",
    "        path_save = os.path.join(path_transformation, channel, targ['tile'])\n",
    "\n",
    "        # Vertical flip & save image\n",
    "        Image.open(path_tobe_transf).transpose(Image.FLIP_TOP_BOTTOM).save(path_save, format=\"TIFF\")\n",
    "\n",
    "        \n",
    "# Zoom in by 1.25\n",
    "tobe_transformed, _ = split_dataset_by_percentage(train_set, percentage)\n",
    "for channel in channel_t:\n",
    "    round = 0\n",
    "    # Transform each image/ channel\n",
    "    for _, targ in tobe_transformed:\n",
    "\n",
    "        # Path to original image\n",
    "        path_tobe_transf = os.path.join(path_original, channel, targ['tile'])\n",
    "\n",
    "        # Path to file directory save\n",
    "        path_transformation = f\"data/{name}/data_clean/data_train/data_transformed/zoom\"\n",
    "        path_save = os.path.join(path_transformation, channel, targ['tile'])\n",
    "\n",
    "        # zoom & save image\n",
    "        zm = clipped_zoom(np.array(Image.open(path_tobe_transf)), 1.25, channel)\n",
    "        zoomed_img = Image.fromarray(zm)\n",
    "        zoomed_img.save(path_save, format=\"TIFF\")\n",
    "\n",
    "# Zoom out by 0.75\n",
    "tobe_transformed, _ = split_dataset_by_percentage(train_set, percentage)\n",
    "for channel in channel_t:\n",
    "    round = 0\n",
    "    # Transform each image/ channel\n",
    "    for _, targ in tobe_transformed:\n",
    "        '''\n",
    "        if round == 10:\n",
    "            break\n",
    "        round+=1'''\n",
    "        # Path to original image\n",
    "        path_tobe_transf = os.path.join(path_original, channel, targ['tile'])\n",
    "\n",
    "        # Path to file directory save\n",
    "        path_transformation = f\"data/{name}/data_clean/data_train/data_transformed/zoom_out\"\n",
    "        path_save = os.path.join(path_transformation, channel, targ['tile'])\n",
    "\n",
    "        # zoom & save image\n",
    "        zm = clipped_zoom(np.array(Image.open(path_tobe_transf),dtype = float), 0.75, channel)\n",
    "        zoomed_img = Image.fromarray(zm)\n",
    "        zoomed_img.save(path_save, format=\"TIFF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f486fb34-6be1-4cfb-bb36-7ffc14673928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change image names so that it can be differentiated to original image \n",
    "channel_t = [\"images\", \"masks\", \"aspect\", \"slope\"]\n",
    "transformation =  [\"h_flipping\", \"v_flipping\", \"zoom\", \"zoom_out\"]\n",
    "path_directory = \"data/Sophia/data_clean/data_train/data_transformed\"\n",
    "for transformation_ in transformation:\n",
    "    for channel in channel_t:\n",
    "        path_d = os.path.join(path_directory, transformation_, channel)\n",
    "        file_list = os.listdir(path_d)\n",
    "        for filename in file_list:\n",
    "            tilename, img_form = os.path.splitext(filename)\n",
    "            if transformation_ == \"h_flipping\":\n",
    "                addon = '_hflip'\n",
    "            elif transformation_ == \"v_flipping\":\n",
    "                addon = '_vflip'\n",
    "            elif transformation_ == \"zoom\":\n",
    "                addon = '_zoom'\n",
    "            elif transformation_ == \"zoom_out\":\n",
    "                addon = '_zoomout'\n",
    "            else:\n",
    "                break\n",
    "            new_filename = f'{tilename}{addon}{img_form}'\n",
    "            old_path = os.path.join(path_d, filename)\n",
    "            new_path = os.path.join(path_d, new_filename)\n",
    "            os.rename(old_path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "113c708e-2873-4ef0-90eb-2358ec1a337c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean unvalid data\n",
    "# Only zoom in can generate unvalid data (No longer contains RTS) -> Clean zoom data\n",
    "\n",
    "#data_root = f'data/{name}/data_clean/data_train/data_original_plus_transformed' #data_transformed/zoom'\n",
    "data_root = 'data/Sophia/data_clean/data_test_tuktoyaktuk'\n",
    "\n",
    "# Clean data\n",
    "toclean_zoom = DEMDataset(data_root, n_channel = 1, channel_list = [\"images\"])\n",
    "original_zoom = DEMDataset(data_root, n_channel = 1, channel_list = [\"images\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "271cd113-7a55-4a10-9263-fe41fcbb8f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-2016_tile_17_40_10_6_8_7.tif image deleted successfully.\n",
      "2010-2016_tile_17_40_10_6_8_7.tif mask deleted successfully.\n",
      "2010-2016_tile_17_40_10_6_8_7.tif aspect deleted successfully.\n",
      "2010-2016_tile_17_40_10_6_8_7.tif slope deleted successfully.\n",
      "2010-2016_tile_17_40_9_7_2_8.tif image deleted successfully.\n",
      "2010-2016_tile_17_40_9_7_2_8.tif mask deleted successfully.\n",
      "2010-2016_tile_17_40_9_7_2_8.tif aspect deleted successfully.\n",
      "2010-2016_tile_17_40_9_7_2_8.tif slope deleted successfully.\n",
      "2010-2016_tile_18_40_2_6_0_0.tif image deleted successfully.\n",
      "2010-2016_tile_18_40_2_6_0_0.tif mask deleted successfully.\n",
      "2010-2016_tile_18_40_2_6_0_0.tif aspect deleted successfully.\n",
      "2010-2016_tile_18_40_2_6_0_0.tif slope deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "clean_zoomed_data(toclean_zoom, original_zoom, data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03bc29aa-2427-4096-a4cc-e323d248034f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copy data over from original\n",
    "root_path = 'data/Sophia/data_clean/data_train/data_original'\n",
    "target_path = 'data/Sophia/data_clean/data_train/data_original_plus_transformed'\n",
    "channel_ = [\"images\", \"masks\", \"aspect\", \"slope\"]\n",
    "for channel in channel_:\n",
    "    path_directory = os.path.join(root_path, channel)\n",
    "    for file_name in  os.listdir(path_directory):\n",
    "        path_origin = os.path.join(root_path, channel,file_name)\n",
    "        path_target = os.path.join(target_path, channel,file_name)\n",
    "        shutil.copy(path_origin, path_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37605537-09e2-4951-a228-63389a272740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copy data over from transformed\n",
    "root_path = 'data/Sophia/data_clean/data_train/data_transformed'\n",
    "target_path = 'data/Sophia/data_clean/data_train/data_original_plus_transformed'\n",
    "transformation = [\"h_flipping\", \"v_flipping\", \"zoom\", \"zoom_out\"]\n",
    "channel_ = [\"images\", \"masks\", \"aspect\", \"slope\"]\n",
    "for transformation_ in transformation:\n",
    "    for channel in channel_:\n",
    "        path_directory = os.path.join(root_path, transformation_, channel)\n",
    "        tot_filename = [item for item in os.listdir(path_directory)\n",
    "                           if item.endswith(\".tif\")]\n",
    "        for file_name in tot_filename:\n",
    "            path_origin = os.path.join(root_path, transformation_, channel,file_name)\n",
    "            path_target = os.path.join(target_path, channel,file_name)\n",
    "            shutil.copy(path_origin, path_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a1b7f-fc48-4e4c-b09c-686725d2abab",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Clean data\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9a6738b-f635-4a19-9f37-fdeea9b2298e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nchannel_t = [\"images\", \"masks\", \"aspect\", \"slope\"]\\ntransformation = [\"h_flipping\", \"v_flipping\", \"zoom\", \"zoom_out\"]\\npath_delete = \"data/Sophia/data_clean/data_train/data_transformed\"\\nfor transformation_ in transformation:\\n    for channel in channel_t:\\n        path_d = os.path.join(path_delete, transformation_, channel)\\n        file_list = os.listdir(path_d)\\n\\n        # Iterate over the files and delete each one\\n        for file_name in file_list:\\n            file_path = os.path.join(path_d, file_name)\\n            try:\\n                if os.path.isfile(file_path):\\n                    os.remove(file_path)\\n                    #print(file_path)\\n                    \\n            except Exception as e:\\n                print(f\"Error deleting {file_path}: {e}\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove bad directories\n",
    "'''\n",
    "channel_t = [\"images\", \"masks\", \"aspect\", \"slope\"]\n",
    "transformation = [\"h_flipping\", \"v_flipping\", \"zoom\", \"zoom_out\"]\n",
    "path_delete = \"data/Sophia/data_clean/data_train/data_transformed\"\n",
    "for transformation_ in transformation:\n",
    "    for channel in channel_t:\n",
    "        path_d = os.path.join(path_delete, transformation_, channel)\n",
    "        file_list = os.listdir(path_d)\n",
    "\n",
    "        # Iterate over the files and delete each one\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(path_d, file_name)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "                    #print(file_path)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting {file_path}: {e}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6647361-1d3d-433b-8117-75465ff83555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# move data\n",
    "import shutil\n",
    "origin = 'data/Sophia/data_clean/data_train/data_original'\n",
    "target = 'data/Sophia/data_clean/data_train/data_original_plus_transformed'\n",
    "shutil.copy(origin, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff0300f-a898-4320-83a4-e0d6f6da94f9",
   "metadata": {},
   "source": [
    "## Circular encoding of aspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "92a6768d-adaa-4a67-8209-6a2a221bd4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_pt_list = [f'data/{name}/data_clean/data_train/data_original', f'data/{name}/data_clean/data_train/data_original_plus_transformed', \n",
    "                f'data/{name}/data_clean/data_train/data_transformed/h_flipping', f'data/{name}/data_clean/data_train/data_transformed/v_flipping', \n",
    "               f'data/{name}/data_clean/data_train/data_transformed/zoom', f'data/{name}/data_clean/data_train/data_transformed/zoom_out',\n",
    "               f'data/{name}/data_clean/data_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "id": "e4ae744c-4feb-424a-80d6-b536a850d20f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for root_pt in root_pt_list:\n",
    "    tile_name = [item for item in os.listdir(os.path.join(root_pt, \"aspect\"))\n",
    "                               if item.endswith(\".tif\")]\n",
    "    round = 0\n",
    "    for tile in tile_name:\n",
    "        '''\n",
    "        if round== 5:\n",
    "            break\n",
    "        round+=1\n",
    "        '''\n",
    "        # path to aspect image\n",
    "        path_tobe_transf = os.path.join(root_pt, \"aspect\",tile)\n",
    "        # Path to file directory save\n",
    "        path_x_aspect = os.path.join(root_pt, 'x_aspect', tile)\n",
    "        path_y_aspect = os.path.join(root_pt, 'y_aspect', tile)\n",
    "\n",
    "        # Get data\n",
    "        matrix_aspect = np.array(Image.open(path_tobe_transf))\n",
    "        # Smooth image to get overall large pattern\n",
    "        smoothed_matrix = gaussian_filter(matrix_aspect, sigma=15)\n",
    "        # Circular encoding\n",
    "        x_aspect = np.cos(smoothed_matrix)\n",
    "        y_aspect = np.sin(smoothed_matrix)\n",
    "\n",
    "        # save image\n",
    "        Image.fromarray(x_aspect).save(path_x_aspect, format=\"TIFF\")\n",
    "        Image.fromarray(y_aspect).save(path_y_aspect, format=\"TIFF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0203e-012b-4bd1-80ce-eabd0e886417",
   "metadata": {},
   "source": [
    "***\n",
    "### Create directory only filled with 2021 data if it is also in 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d526ef27-7646-47a1-8d9b-85581afa4db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "channel_list = ['images', 'masks', 'slope']\n",
    "dir1 = \"data/Sophia/data_clean/data_test\"\n",
    "dir2 = \"data/Sophia/data_clean/2010_2021\"\n",
    "dir3 = \"data/Sophia/data_clean/2021_also_in_2016_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e928d-ecfc-44c7-a1fd-b42f4550c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy file from dir2 to dir3 if name is in dir1\n",
    "for channel in channel_list:\n",
    "    for filename in os.listdir(os.path.join(dir1, channel)):\n",
    "        filename10_21 = '2010-2021' + filename[9:]\n",
    "        file_path_copy = os.path.join(dir2, channel, filename10_21)\n",
    "        filename11_21 = '2011-2021' + filename[9:]\n",
    "        file_path_copy11 = os.path.join(dir2, channel, filename11_21)\n",
    "\n",
    "        if os.path.exists(file_path_copy):\n",
    "            shutil.copy(file_path_copy, os.path.join(dir3, channel))\n",
    "        elif os.path.exists(file_path_copy11):\n",
    "            shutil.copy(file_path_copy11, os.path.join(dir3, channel))\n",
    "        #else:\n",
    "            #print(f\"File '{filename[9:]}' not found in {dir2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba347ed-602f-47b2-b1a0-9f3cdb8ec7a8",
   "metadata": {},
   "source": [
    "***\n",
    "# min-max normalize data \n",
    "### Get global min, max for min-max scaling (Currently only of clean data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "id": "3c909355-c73f-4741-b65a-d78369d2e9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channel_list = ['aspect','images', 'masks', 'slope', 'x_aspect', 'y_aspect'] # , 'images', 'masks', 'slope', 'x_aspect', 'y_aspect'\n",
    "aspect_min = np.nan\n",
    "aspect_max = np.nan\n",
    "images_min = np.nan\n",
    "images_max = np.nan\n",
    "masks_min = np.nan\n",
    "masks_max = np.nan\n",
    "slope_min = np.nan\n",
    "slope_max = np.nan\n",
    "x_aspect_min = np.nan\n",
    "x_aspect_max = np.nan\n",
    "y_aspect_min = np.nan\n",
    "y_aspect_max = np.nan\n",
    "\n",
    "\n",
    "root_pt_list = ['data/Sophia/data_clean/data_train/data_original', 'data/Sophia/data_clean/data_test']\n",
    "\n",
    "\n",
    "for channel in channel_list:\n",
    "    for root in root_pt_list:\n",
    "        tile_name = [item for item in os.listdir(os.path.join(root, channel))\n",
    "                               if item.endswith(\".tif\")]\n",
    "        for i, tile in enumerate(tile_name):\n",
    "            tile_path =os.path.join(root, channel, tile)\n",
    "            img = np.array(Image.open(tile_path))\n",
    "            min_local = np.min(img)\n",
    "            max_local = np.max(img)\n",
    "            if i ==0: # first image cannot overwrite np.nan\n",
    "                if channel == 'aspect':\n",
    "                    aspect_min = min_local\n",
    "                    aspect_max = max_local\n",
    "                elif channel == 'images':\n",
    "                    images_min = min_local\n",
    "                    images_max = max_local\n",
    "                elif channel == 'masks':\n",
    "                    masks_min = min_local\n",
    "                    masks_max = max_local\n",
    "                elif channel == 'slope':\n",
    "                    slope_min = min_local\n",
    "                    slope_max = max_local\n",
    "                elif channel == 'x_aspect':\n",
    "                    x_aspect_min = min_local\n",
    "                    x_aspect_max = max_local\n",
    "                elif channel == 'y_aspect':\n",
    "                    y_aspect_min = min_local\n",
    "                    y_aspect_max = max_local\n",
    "                \n",
    "\n",
    "            else: # one has to check if local max/ min is global max/min\n",
    "                if channel == 'aspect':\n",
    "                    if min_local < aspect_min:\n",
    "                        aspect_min = min_local\n",
    "                    if max_local > aspect_max:\n",
    "                        aspect_max = max_local\n",
    "                elif channel == 'images':\n",
    "                    if min_local < images_min:\n",
    "                        images_min = min_local\n",
    "                    if max_local > images_max:\n",
    "                        images_max = max_local\n",
    "                elif channel == 'masks':\n",
    "                    if min_local < masks_min:\n",
    "                        masks_min = min_local\n",
    "                    if max_local > masks_max:\n",
    "                        masks_max = max_local\n",
    "                elif channel == 'slope':\n",
    "                    if min_local < slope_min:\n",
    "                        slope_min = min_local\n",
    "                    if max_local > slope_max:\n",
    "                        slope_max = max_local\n",
    "                elif channel == 'x_aspect':\n",
    "                    if min_local < x_aspect_min:\n",
    "                        x_aspect_min = min_local\n",
    "                    if max_local > x_aspect_max:\n",
    "                        x_aspect_max = max_local\n",
    "                elif channel == 'y_aspect':\n",
    "                    if min_local < y_aspect_min:\n",
    "                        y_aspect_min = min_local\n",
    "                    if max_local > y_aspect_max:\n",
    "                        y_aspect_max = max_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "id": "6d305cc6-c306-438e-8aa5-6ee23d0d2b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data  = {'aspect_min': [aspect_min], 'aspect_max': [aspect_max], 'images_min': [images_min], 'images_max': [images_max], 'masks_min':[masks_min], 'masks_max':[masks_max],\n",
    "         'slope_min': [slope_min], 'slope_max': [slope_max], 'x_aspect_min':[x_aspect_min],\n",
    "         'x_aspect_max': [x_aspect_max],'y_aspect_min': [y_aspect_min], 'y_aspect_max': [y_aspect_max], 'x_aspect_max': [x_aspect_max], 'y_aspect_min': [y_aspect_min],'y_aspect_max': [y_aspect_max]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "id": "75442221-d7fe-4c51-a17c-822e8c3b693f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_max = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "3a734058-7f9a-4498-b2c4-b7f098037011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_max.to_csv('min_max.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e88aa05-90bf-4f05-94b3-7b2d2aa6f67b",
   "metadata": {},
   "source": [
    "### Min max normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "id": "3bd2a8c7-0af5-4b49-b2aa-ebea02059712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_pt_list = ['data/Sophia/data_clean/data_train/data_original', 'data/Sophia/data_clean/data_train/data_original_plus_transformed',\n",
    "           'data/Sophia/data_clean/data_test']\n",
    "destination_pth = ['data/Sophia/data_clean/data_train/data_original_scaled', 'data/Sophia/data_clean/data_train/data_original_plus_transformed_scaled',\n",
    "                  'data/Sophia/data_clean/data_test_scaled']\n",
    "channel_list = ['images', 'masks', 'slope', 'x_aspect', 'y_aspect']\n",
    "\n",
    "for root_i, root in enumerate(root_pt_list):\n",
    "    for channel in channel_list:\n",
    "        tile_name = [item for item in os.listdir(os.path.join(root, channel))\n",
    "                               if item.endswith(\".tif\")]\n",
    "        for i, tile in enumerate(tile_name):\n",
    "            tile_path =os.path.join(root, channel, tile)\n",
    "            img = np.array(Image.open(tile_path))\n",
    "            if channel == 'images':\n",
    "                images_min = min_max['images_min'][0]\n",
    "                images_max = min_max['images_max'][0]\n",
    "                img_transf = (img-images_min)/(images_max-images_min)\n",
    "\n",
    "                path_save = os.path.join(destination_pth[root_i], channel, tile)\n",
    "                Image.fromarray(img_transf).save(path_save, format=\"TIFF\")\n",
    "\n",
    "            elif channel == 'masks': # no mormalization needed, is label channel\n",
    "                path_save = os.path.join(destination_pth[root_i], channel, tile)\n",
    "                Image.fromarray(img).save(path_save, format=\"TIFF\")\n",
    "\n",
    "            elif channel == 'slope':\n",
    "                slope_min = min_max['slope_min'][0]\n",
    "                slope_max = min_max['slope_max'][0]\n",
    "                img_transf = (img-slope_min)/(slope_max-slope_min)\n",
    "                path_save = os.path.join(destination_pth[root_i], channel, tile)\n",
    "                Image.fromarray(img_transf).save(path_save, format=\"TIFF\")\n",
    "\n",
    "            elif channel == 'x_aspect':\n",
    "                x_aspect_min = min_max['x_aspect_min'][0]\n",
    "                x_aspect_max = min_max['x_aspect_max'][0]\n",
    "                img_transf = (img-x_aspect_min)/(x_aspect_max-x_aspect_min)\n",
    "                path_save = os.path.join(destination_pth[root_i], channel, tile)\n",
    "                Image.fromarray(img_transf).save(path_save, format=\"TIFF\")\n",
    "            elif channel == 'y_aspect':\n",
    "                y_aspect_min = min_max['y_aspect_min'][0]\n",
    "                y_aspect_max = min_max['y_aspect_max'][0]\n",
    "                img_transf = (img-y_aspect_min)/(y_aspect_max-y_aspect_min)\n",
    "                path_save = os.path.join(destination_pth[root_i], channel, tile)\n",
    "                Image.fromarray(img_transf).save(path_save, format=\"TIFF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df209f59-e217-4dda-8df1-a05a03c24578",
   "metadata": {},
   "source": [
    "### Get global mean, sd of normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0cd1a806-720c-4b59-9b61-1c59a0f70e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channel_list = ['images', 'slope'] #, 'x_aspect', 'y_aspect']\n",
    "\n",
    "img_data = []\n",
    "slope_data = []\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "path_data = ['data/Sophia/data_clean/data_test_tuktoyaktuk'] #['data/Sophia/data_clean/data_train/data_original']\n",
    "\n",
    "for path_d in path_data:\n",
    "    for channel in channel_list:\n",
    "        tile_name = [item for item in os.listdir(os.path.join(path_d, channel))\n",
    "                               if item.endswith(\".tif\")]\n",
    "        for i, tile in enumerate(tile_name):\n",
    "            matrix = np.array(Image.open(os.path.join(path_d, channel, tile)))            \n",
    "            if channel == 'images':\n",
    "                img_data.append(matrix)\n",
    "            elif channel == 'slope':\n",
    "                slope_data.append(matrix)\n",
    "            elif channel == 'x_aspect':\n",
    "                x_data.append(matrix)\n",
    "            elif channel == 'y_aspect':\n",
    "                y_data.append(matrix)\n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c3f3471c-ae52-4997-85b2-4979059eb8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_tot = np.concatenate(img_data)\n",
    "img_sd = np.nanstd(img_tot)\n",
    "img_mean = np.nanmean(img_tot)\n",
    "\n",
    "slope_tot = np.concatenate(slope_data)\n",
    "slope_sd = np.nanstd(slope_tot)\n",
    "slope_mean = np.nanmean(slope_tot)\n",
    "\n",
    "x_tot = np.concatenate(x_data)\n",
    "x_sd = np.nanstd(x_tot)\n",
    "x_mean = np.nanmean(x_tot)\n",
    "\n",
    "y_tot = np.concatenate(y_data)\n",
    "y_sd = np.nanstd(y_tot)\n",
    "y_mean = np.nanmean(y_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "13a521f1-bf3f-4d40-8e74-b0f5ff8193df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'img_sd': [img_sd],\n",
    "    'img_mean': [img_mean],\n",
    "    'slope_sd': [slope_sd],\n",
    "    'slope_mean': [slope_mean],\n",
    "    'x_sd': [x_sd],\n",
    "    'x_mean': [x_mean],\n",
    "    'y_sd': [y_sd],\n",
    "    'y_mean': [y_mean]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "mean_sd = pd.DataFrame(data)\n",
    "mean_sd.to_csv('mean_sd_tukto.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd6b3f1-02d3-4bf9-9122-4082bfbd3083",
   "metadata": {},
   "source": [
    "## Create directory only filled with 2021 data if it is also in 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "5225f48e-879d-47a7-8d48-1232fabd56cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "channel_list = ['images', 'masks', 'slope']\n",
    "dir1 = \"data/Sophia/data_clean/data_test\"\n",
    "dir2 = \"data/Sophia/data_clean/2010_2021\"\n",
    "dir3 = \"data/Sophia/data_clean/2021_also_in_2016_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "725cddd8-1bc4-4104-8a4e-2eb193b84581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for channel in channel_list:\n",
    "    for filename in os.listdir(os.path.join(dir1, channel)):\n",
    "        filename10_21 = '2010-2021' + filename[9:]\n",
    "        file_path_copy = os.path.join(dir2, channel, filename10_21)\n",
    "        filename11_21 = '2011-2021' + filename[9:]\n",
    "        file_path_copy11 = os.path.join(dir2, channel, filename11_21)\n",
    "\n",
    "        if os.path.exists(file_path_copy):\n",
    "            shutil.copy(file_path_copy, os.path.join(dir3, channel))\n",
    "        elif os.path.exists(file_path_copy11):\n",
    "            shutil.copy(file_path_copy11, os.path.join(dir3, channel))\n",
    "        #else:\n",
    "            #print(f\"File '{filename[9:]}' not found in {dir2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
